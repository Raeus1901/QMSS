# -*- coding: utf-8 -*-
"""HMW 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17yYlKBMn_YD3l5iKaYeJEVXb_T0NeZ7X

# Part 1: Regression on California Test Scores

1. Find the url for the California Test Score Data Set from the following website:
"""

#Import Python Libraries
import numpy as np
import scipy as sp
import pandas as pd
import matplotlib as mpl
import seaborn as sns
import sklearn as skl

cali_df = pd.read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Caschool.csv")

cali_df.dtypes

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from matplotlib import pyplot as plt

# Univariate distribution of the target feature: testscr
sns.histplot(cali_df['testscr'], kde=True)
plt.title('Average test score performance for 5th grade Californians')
plt.show()

variables =  ['avginc', 'str', 'enrltot']

for var in variables:
    sns.histplot(cali_df[var], kde=True)
    description = {
        'avginc': 'Average Income in K',
        'str': 'Student-Teacher Ratio',
        'enrltot': 'Total Enrollment'
    }
    plt.title(f'Distribution of {description[var]}')
    plt.show()

"""### 1.2 Visualize the dependency of the target on each feature from 1.1."""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from matplotlib import pyplot as plt

variables =  ['avginc', 'str', 'enrltot']


for var in variables:
  plt.figure(figsize=(10, 6))
  sns.regplot(x=cali_df[var], y=cali_df['testscr'])
  plt.title(f'Relationship between Test Score and {description[var]} with Regression Line')
  plt.xlabel(description[var])
  plt.ylabel('Test Score')
  plt.show()

"""There is notable positive relationship associated with higher income causing better test score. Relationship seems more accurate than others given a smaller fit and clear 45Â° degree line. The two others are more problematic due to possible overfit given larger space, and an unclear direction for the line.

### 1.3 Split data in training and test set. Build models that evaluate the relationship between all available X variables in the California test dataset and the target variable. Evaluate KNN for regression, Linear Regression (OLS), Ridge, and Lasso using cross-validation with the default parameters. Does scaling the data with the StandardScaler help?

First, let's clean the data. We separate the target features (and its extension), as we remove the object type columns given that the program can only load float (numerical) values.
"""

from sklearn.model_selection import train_test_split

# Assuming X is your feature matrix and y is the target vector

# I remove all the objects columns + the variables affecting the target
X = pd.get_dummies(cali_df, columns=['county', 'district', 'grspan']).drop(columns=['readscr', 'mathscr', 'testscr'])
y = cali_df['testscr']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

print(y[0:10])
X_train.head()

from sklearn.neighbors import KNeighborsRegressor

knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, y_train)

#Print accuracy rounded to two digits to the right of decimal
print(knn.score(X_test, y_test))
y_pred = knn.predict(X_test)

"""Low Knn, could be better"""

from sklearn.model_selection import train_test_split
import statsmodels.api as sm

X_train_new = sm.add_constant(X_train)

model = sm.OLS(y_train, X_train_new).fit()
print(model.summary())

from sklearn.linear_model import Ridge
ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)
print("Training set score: {:.2f}".format(ridge01.score(X_train, y_train)))
print("Test set score: {:.2f}".format(ridge01.score(X_test, y_test)))

from sklearn.linear_model import Lasso

# Lower alpha to fit a more complex model
# we increase the default setting of "max_iter",
# otherwise the model would warn us that we should increase max_iter.

lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)
print("Training set score: {:.2f}".format(lasso001.score(X_train, y_train)))
print("Test set score: {:.2f}".format(lasso001.score(X_test, y_test)))
print("Number of features used: {}".format(np.sum(lasso001.coef_ != 0)))

"""Both Lasso and Ridge seem equally as accurate.

#### Now Let's use standard scaler
"""

from sklearn import preprocessing

X_train, X_test, y_train, y_test = train_test_split(X, y)

scaler = preprocessing.StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
scaler.transform(X_test)

#OLS

X_train_scaled_new = sm.add_constant(X_train_scaled)

model = sm.OLS(y_train, X_train_scaled_new).fit()
print(model.summary())

print("KNN + RIDGE + LASSO")

#KNN

knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train_scaled, y_train)
print(knn.score(X_test_scaled, y_test), "=> KNN")
y_pred = knn.predict(X_test_scaled)

#Ridge

ridge01 = Ridge(alpha=0.1).fit(X_train_scaled, y_train)
print("Training set score: {:.2f}".format(ridge01.score(X_train_scaled, y_train), "=> Ridge"))
print("Test set score: {:.2f}".format(ridge01.score(X_test_scaled, y_test), "=> Ridge"))

#Lasso

lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train_scaled, y_train)
print("Training set score: {:.2f}".format(lasso001.score(X_train_scaled, y_train), "=> Lasso"))
print("Test set score: {:.2f}".format(lasso001.score(X_test_scaled, y_test), "=> Lasso"))
print("Number of features used: {}".format(np.sum(lasso001.coef_ != 0)))

"""Scaling results were mixed given an increase the accuracy for the training set and test score of  it reaches a perfect 1 value for Lasso and Ridge. However, this comes with the risk of more overfit given a decrease in the test score for both, notably Ridge. KNN performance is very esticatic, as it can have higher accuracy coefficient much like a very low one. The OLS regression is estatic, with. The model is worse after scaling due to its complex nature attributed to a large dataset, hyperparameters of different nature so rendering the dataset very sensitive to external noises; increasing its tendency to overfit.

### 1.4 Tune the parameters of the models where possible using GridSearchCV. Do the results improve?
"""

from sklearn.pipeline import make_pipeline
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler

knn_pipe = make_pipeline(StandardScaler(), KNeighborsRegressor())
print(knn_pipe.steps) # names in single quotes (i.e.-'standardscaler' and 'kneighborsregressor')

"""The above values have only two steps as in simarly to a binary model. Now we have removed the mean and scale to unit variance, let's see how it goes for KNN. Recall that GridSeach can't be used for linear models due to their absence of multiples parameters. It could be for Lasso and Ridge using penalized parameters, which we did not in this exercise.  
. I won't use the scaled variables from standard scaller as GridSearch already does it albeit by removing the means and converting to variance.   
"""

knn_pipe = make_pipeline(StandardScaler(), KNeighborsRegressor())

#refer to step name with two underscores before argument name when...
#you build a parameter grid

param_grid_knn = {'kneighborsregressor__n_neighbors': range(1, 10)}
grid = GridSearchCV(knn_pipe, param_grid_knn, cv=10)
grid.fit(X_train, y_train)
print(grid.best_params_)
print(grid.score(X_test, y_test))

"""Knn model is better, with an increase in accuracy points compared to the first model. GridSearch is more accurate as it written specifically to scale data with hyperparameters, that is multidimensional. The previous scaling method was inacurate because it tried to fit a multilinear model into a linear one. GridSearch shows that the model is better perfoming with 2 neighboors.

I will use a pipeline scaling method with regular transformation for Lasso and Ridge.
"""

from sklearn.pipeline import make_pipeline

pipe = make_pipeline(StandardScaler(), Ridge())
pipe.fit(X_train, y_train)
pipe.score(X_test, y_test)

from sklearn.pipeline import make_pipeline

pipe = make_pipeline(StandardScaler(), Lasso())
pipe.fit(X_train, y_train)
pipe.score(X_test, y_test)

"""### 1.5 Compare the coefficients of your two best linear models (not knn), do they agree on which features are important?

It was hard to tell which model between Lasso and Ridge have had a better performance in its training and test set given that both always had similar reuslts. However this changes when scaling: 1) Standard Scaler increases more the accuracy of Lasso than Ridge respectively from 0.8 to 0.83 / 0.81 to 0.78; 2) Pipeline scaling Lasso has a better accuracy over Ridge by a margin of 0.12 points. So Lasso is the most accurate.
Because of their closer score, then the model follow an accurate depiction in determining which features influence the most the target variable.

###1.6 Now that you have experimented with different models, discuss which final model you would choose to predict new data

Lasso, without a doubt. It better performs than both models. Why is it more accurate comes from its shrinkage to 0 of the variables than Ridge which does it a lighter way, while defining itself as a linear model; giving an edge of stability over Knn, which could intepret features in a wrong way due to its hyperparameters especially as the model gets more complex. On a side note, Knn does not use training data based upon simple matrixes, making it less reliable than Lasso and Ridge in a scenario where data is spread out, and unorganized. This is the case with that dataset given its float, int and objects features format which decreases the efficiency in the machine to correclty interpret some features rigourously. To sum up, Lasso ability's to shrink to 0 all the most irrelevant features is what makes it stands out over the others, especially after the data has been scaled.

# Part 2: Classification on red and white wine characteristics
"""

pip install ucimlrepo

from google.colab import drive

   drive.mount('/content/drive')

import pandas as pd

red_wine_df = pd.read_csv("/content/drive/My Drive/winequality-red.csv", sep = ';')
red_wine_df['winetype'] = 1

white_wine_df = pd.read_csv("/content/drive/My Drive/winequality-white.csv", sep = ';')
white_wine_df['winetype'] = 0

wine_df = pd.concat([red_wine_df, white_wine_df])
print(wine_df.head())

wine_df.dtypes

"""### 2.1 Visualize the univariate distribution of the target feature and each of the three explanatory variables that you think are likely to have a relationship with the target feature.   """

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
# %matplotlib inline
from matplotlib import pyplot as plt

# Univariate distribution of the target feature: testscr
sns.histplot(wine_df['winetype'], kde=True)
plt.title('univariate distribution to the type of wine')
plt.xticks(ticks=[0,1], labels=['White Wine', 'Red Wine'])
plt.show()

variables =  ['density', 'pH', 'residual sugar']

for var in variables:
    sns.histplot(wine_df[var], kde=True)
    description = {
        'density': 'wine',
        'pH': 'average pH',
        'residual sugar': 'average sugar'
    }
    plt.title(f'Distribution of {description[var]}')
    plt.show()

"""### 2.2 Split data into training and test set. Build models that evaluate the relationship between all available X variables in the dataset and the target variable. Evaluate Logistic Regression, Penalized Logistic Regression, and KNN for classification using cross-validation. How different are the results? How does scaling the data with StandardScaler influence the results?"""

from sklearn.model_selection import train_test_split

# Assuming X is your feature matrix and y is the target vector

# Use train_test_split(X,y) to create four new data sets, defaults to .75/.25 split
X = wine_df.loc [:,wine_df.columns != 'winetype']
y = wine_df['winetype']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

print(y[0:10])
X_train.head()

print("Training set X size:", X_train.shape)
print("Test set X size:", X_test.shape)
print("Training set y size:", y_train.shape)
print("Test set y size:", y_test.shape)

"""I use various logistic regression model to predict which one is the most accurate. I will first begin with an ordinary logistic regression model (which I assume will be the less accurate), then penalized models with Lasso followed by Ridge; both of them will have changes in their C parameter, eventually to finish with a Knn model.

I chose to begin with regular log, then Ridge because it uses a more moderate model, with squared penalty in shrinking the coefficient but not to zero. Lasso uses absolute penalty, can shrink some coefficients to zero. Lasso is more likely to create a better fit but at the risk of underfit. I won't include a multivariate model given the analysis remains binary.

#### -> Ordinary model
"""

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(penalty='none').fit(X_train, y_train)

print("logreg .coef_: {}".format(logreg .coef_))
print("Training set score: {:.3f}".format(logreg.score(X_train, y_train)))
print("Test set score: {:.3f}".format(logreg.score(X_test, y_test)))

predicted_vals = logreg.predict(X_test) # y_pred includes your predictions
print("logreg.predict: {}".format(predicted_vals))

"""The model does perfom well given a difference of 0.006 between the training and test score, meaning the algorithm could have an easier time identifying white or red wine thanks to the independent features. There is still some degree of overfit; as the next models will attempt to reduce this gap, and make the model almost entirely accurate with a low margin of errors."""

import statsmodels.api as sm
#remember still need to add column of 1's
X_train_new = sm.add_constant(X_train)

#remember that y is first and then X in statsmodel; Generalized Linear Model and binomial family for Logistic regression
model = sm.GLM(y_train, X_train_new, family=sm.families.Binomial()).fit()

model.summary()

"""Now, let's include a penalty to reduce the impact of the independent features defining the type of dependent wine (there is too many attributes which constrain the ability of the model to predict whichever wine is red/white as they require to be dampened => Ridge using two set of C1 parameters)"""

# Smaller C will constrain Betas more.  It's a tuning parameter we can find using gridsearch.
# Lowering C will make coefficients larger, larger C will make coefficients smaller
# Note: L2 will shrink coefficients down, never reaching 0. L1 has potential to zero out coefficients

#C=1, compare coefs to regular model above.
logreg = LogisticRegression(C=100, penalty='l2').fit(X_train, y_train)

print("logreg .coef_: {}".format(logreg .coef_))
print("Training set score: {:.3f}".format(logreg.score(X_train, y_train)))
print("Test set score: {:.3f}".format(logreg.score(X_test, y_test)))


predicted_vals = logreg.predict(X_test) # y_pred includes your predictions
print("logreg.predict: {}".format(predicted_vals))

"""Same score difference albeilt with higher log coefficient. Not too much of an improvement"""

logreg = LogisticRegression(C=0.01, penalty='l2').fit(X_train, y_train)
print("logreg .coef_: {}".format(logreg .coef_))
print("Training set score: {:.3f}".format(logreg.score(X_train, y_train)))
print("Test set score: {:.3f}".format(logreg.score(X_test, y_test)))

predicted_vals = logreg.predict(X_test)
print("logreg.predict: {}".format(predicted_vals))

"""###### The Logistic model with a lower C parameter is more overfitting, performs worse on the sets and has a harder time predicting the type of wine given smaller log coefficients. Not the best model.

###### -> Let's move on to Lasso model (l1)
"""

logreg = LogisticRegression(C=.01, penalty='l1',solver='liblinear').fit(X_train, y_train)

print("logreg .coef_: {}".format(logreg .coef_))
print("Training set score: {:.3f}".format(logreg.score(X_train, y_train)))
print("Test set score: {:.3f}".format(logreg.score(X_test, y_test)))

predicted_vals = logreg.predict_proba(X_test) # y_pred includes your predictions
print("logreg.predict: {}".format(predicted_vals))

"""###### Compared to the regular C1=0.01 model, Lasso increased the coefficient by 10 points but with a  worse performance on the training/test score. However, the features prediction have shrunk so much that their impact is much more moderate, rendering the model less accurate"""

logreg = LogisticRegression(C=100, penalty='l1',solver='liblinear').fit(X_train, y_train)

print("logreg .coef_: {}".format(logreg .coef_))
print("Training set score: {:.3f}".format(logreg.score(X_train, y_train)))
print("Test set score: {:.3f}".format(logreg.score(X_test, y_test)))

predicted_vals = logreg.predict_proba(X_test) # y_pred includes your predictions
print("logreg.predict: {}".format(predicted_vals))

"""Lasso is more efficient using a C1 parameter with a higher value; perhaps because regularization is weaker which causes less shrinking in the features values, so much more degree of accuracy. Yet in the case of C=100 with l1 the model attempt to shrink the coefficients to 0 which it cannot, rendering the features to have a stronger effect on the dependent variable regression. With a difference of 0.003, this model performs exceptionally well compared to the others as it manages to better capture the new data through generalization; whereas the others would overfit the model with more information.

#### So far, this seems to be most efficient model.

Time to use Knn now. This is different from the previous model as Knn use dataset distance score between the training variables instaed of an attriubte analysis. This could makes the model less accurate in a larger dataset. For simplicity, I will use 10 neighboors.
"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(X_train, y_train)

#Print accuracy rounded to two digits to the right of decimal
print(knn.score(X_test, y_test))
y_pred = knn.predict(X_test)

"""Model accuracy is 93.4%"""

y_pred

#import cross validation functions from sk learn

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold

# Set up function parameters for diff't cross validation strategies
kfold = KFold(n_splits=5)
skfold = StratifiedKFold(n_splits=5, shuffle=True)
rkf = RepeatedKFold(n_splits=5, n_repeats=10)

print("KFold: "+str(cross_val_score(KNeighborsClassifier(n_neighbors=10), X_train, y_train, cv=kfold).mean()))

print("StratifiedKFold:\n{}".format(
cross_val_score(KNeighborsClassifier(n_neighbors=5), X_train, y_train, cv=skfold).mean()))

print("RepeatedKFold:\n{}".format(
cross_val_score(KNeighborsClassifier(n_neighbors=5), X_train, y_train,  cv=rkf).mean()))

"""The model is now more accurate by using 5 neighboors. The model seem to be resilient in capturing random noises, succesfully providing a good accuracy in evaluating wine type (alos capturing new data related to wine). With an average of 94%, the model is strong and promising but stills performs less than the Lasso C1=100 model.

Now Let's test using standard scale on normal logistic regression, l1 with c=100 and kFold.
"""

from sklearn.linear_model import LogisticRegression
from sklearn import preprocessing

X_train, X_test, y_train, y_test = train_test_split(X, y)

scaler = preprocessing.StandardScaler().fit(X_train)

X_train_scaled = scaler.transform(X_train)
scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
logreg.score(X_test_scaled, y_test)

logreg = LogisticRegression(penalty='none').fit(X_train_scaled, y_train)

print("logreg .coef_: {}".format(logreg .coef_))
print("Training set score: {:.3f}".format(logreg.score(X_train_scaled, y_train)))
print("Test set score: {:.3f}".format(logreg.score(X_test_scaled, y_test)))

predicted_vals = logreg.predict(X_test_scaled) # y_pred includes your predictions
print("logreg.predict: {}".format(predicted_vals))

X_train, X_test, y_train, y_test = train_test_split(X, y)

scaler = preprocessing.StandardScaler().fit(X_train)

X_train_scaled = scaler.transform(X_train)
scaler.transform(X_train)
logreg = LogisticRegression(C=100, penalty='l1',solver='liblinear').fit(X_train_scaled, y_train)

X_test_scaled = scaler.transform(X_test)
logreg.score(X_test_scaled, y_test)

print("logreg .coef_: {}".format(logreg .coef_))
print("Training set score: {:.3f}".format(logreg.score(X_train_scaled, y_train)))
print("Test set score: {:.3f}".format(logreg.score(X_test_scaled, y_test)))

predicted_vals = logreg.predict_proba(X_test_scaled) # y_pred includes your predictions
print("logreg.predict: {}".format(predicted_vals))

X_train, X_test, y_train, y_test = train_test_split(X, y)

scaler = preprocessing.StandardScaler().fit(X_train)

X_train_scaled = scaler.transform(X_train)
scaler.transform(X_train)


kfold = KFold(n_splits=5)
skfold = StratifiedKFold(n_splits=5, shuffle=True)
rkf = RepeatedKFold(n_splits=5, n_repeats=10)

print("KFold: "+str(cross_val_score(KNeighborsClassifier(n_neighbors=10), X_train_scaled, y_train, cv=kfold).mean()))

print("StratifiedKFold:\n{}".format(
cross_val_score(KNeighborsClassifier(n_neighbors=5), X_train_scaled, y_train, cv=skfold).mean()))

print("RepeatedKFold:\n{}".format(
cross_val_score(KNeighborsClassifier(n_neighbors=5), X_train_scaled, y_train,  cv=rkf).mean()))

"""Lasso with C1=100 remains the most accurate with a test score between 0.994 to 0.998, closely followed by stratiffied Knn with a score of 0.9938. Notice that Lasso can be object to underfit as its test score can be some cases higher than the training score.

### 2.3 Tune the parameters where possible using GridSearchCV. Do the results improve?

First, I implement an odd numbers array beginning with 1 and finishing with 13, taking two steps. I voluntarily finished with 15 to keep the list with odd numbers.
"""

import numpy as np

np.arange(1, 15, 2)

"""Then, I use this array on my Knn model to better capture the strenght of the hyperparameters in affecting the relationship between the k = 13 neighboors in the dataset. I'll train the model 20 times to maximizes its efficiency."""

from sklearn.model_selection import GridSearchCV
import numpy as np

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)

#create dictionary data object with keys equal to parameter name 'n_neighbors'
#for knn model and values equal to range of k values to create models for

param_grid = {'n_neighbors': [1,3,5,7,9,10,11,13] }#np.arange creates sequence of numbers for each k value

grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=20)

#use meta model methods to fit score and predict model:
grid.fit(X_train, y_train)

#extract best score and parameter by calling objects "best_score_" and "best_params_"
print("best mean cross-validation score: {:.3f}".format(grid.best_score_))
print("best parameters: {}".format(grid.best_params_))
print("test-set score: {:.3f}".format(grid.score(X_test, y_test)))

"""Test score is of 0.942 which is solid and accurate given how close it is to the previous cross-validation score of 0.9419. The best parameter is for 1 neighboor which is a good sign as the model is able to capture noisey signals, able to differentiate each attributes differently and to identify new neighboors in the model. In other words, the machine is learning efficiently and proves to be resilient given the high accuracy score with 20 attempts. It still remains less predictive than the Lasso, but present less risk of overfit.

Let's try with Lasso using GridSearch, with multiple C values.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression


logreg = LogisticRegression(penalty='l1', solver='liblinear')
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
}

grid = GridSearchCV(logreg, param_grid=param_grid, cv=20)
grid.fit(X_train, y_train)

# Display results
print("best mean cross-validation score: {:.3f}".format(grid.best_score_))
print("best parameters: {}".format(grid.best_params_))
print("test-set score: {:.3f}".format(grid.score(X_test, y_test)))

"""Lasso performs much better than stratified Knn with a best paramter of C=100. The risk of underfit disapears as the alpha value gets higher.

### 2.4 Change the cross-validation strategy in GridSearchCV from âstratified k-foldâ to âkfoldâ with shuffling. Do the parameters for models that can be tuned change? Or if you change the random state of the split into training and test data
"""

from sklearn.model_selection import GridSearchCV, KFold
import numpy as np

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)

# Create dictionary data object with keys equal to parameter name 'n_neighbors'
# for knn model and values equal to range of k values to create models for
param_grid = {'n_neighbors': [1,3,5,7,9,10,11,13]}

# Set up KFold cross-validation strategy with shuffling
kfold = KFold(n_splits=20, shuffle=True, random_state=None)

grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=kfold)

# Use meta model methods to fit score and predict model
grid.fit(X_train, y_train)

# Extract best score and parameter
print("best mean cross-validation score: {:.3f}".format(grid.best_score_))
print("best parameters: {}".format(grid.best_params_))
print("test-set score: {:.3f}".format(grid.score(X_test, y_test)))

"""The optimal hyperparameters change depending on several factors, including the cross-validation strategy and the specific train-test split determined by the random_state as shown above. The best average score remains consistent with the previous analysis with a difference of 0.005 indicating the parameters are consistent within the Knn model. The test score change is smaller with 0.006, showing that changing the random state to train_test_split could lead to higher variations than with the use of new parameters added to the model at the risk of increasing the model accuracy by 0.010 points. Again, this analysis remains consistent with the previous model showing that the best parameter is always 1 neighboor; affirming that changing the distance between the neighboors in the training data will affect more the model than than changing their attributes (=>parameters). Finally, hyperparameters are the most resilient to changes in the model given the k optimal score of 1; as the machine is able to identify all the underlying, hidden structures in the dataset. The main worries come from adding wines with less specication such as RosÃ© or Orange, which would be very distant from red/white wine attributes as identified by the model.

2.5 Lastly, discuss which final model you would choose to predict new data.

I believe Lasso with C1=100 is the most efficient (with GridSearch) for multiple reasons
  1) is the most accurate: With a score of 98.6%, it is the most accurate in predicting the type of wine given the independent variables. 2) Is the most stable, given its accuracy score remains consistent and high after scalling its parameters and the data with different methods. 3) It can captures hypersensitivity thanks to its alpha parameter. 4) The main issue was underfit, but it has been fixed with GridSearch
  On the long-run, the stratified knn is also adequate but more prone to variations than Lasso. So I stick with Lasso.
"""
