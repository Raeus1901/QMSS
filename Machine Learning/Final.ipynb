{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Final"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.  From the perspective of a social scientist, which models did we learn this semester that are useful for ruling out alternative explanations through control variables AND that allow us to observe substantively meaningful information from model coefficients?\n",
        "\n",
        "Let's start by defining the meaning of each statement:\n",
        "\n",
        "1)  \"ruling out alternative explanations through control variables\": Implies creating a sustainaible training environment free of exogeneous variables that could potentially counfound the accuracy result given by the test set. Such problem can be misleading as the concluding result might not be representative what the researcher seeks to achieve in identidying the relastionship of the  explanatory variables to the dependent outcome. As such, control variables are added to isolate the dependent variable from unexplained variables, mimizing alternative explanations. For example, in studying the impact of education level on income, factors such as age, gender, and work experience might also influence income. These would be included as control variables in the model to ensure that the observed effect of education on income is not confounded by these other factors.\n",
        "\n",
        "2) \"observing substantively meaningful information from model coefficients\" : The coefficients obtained from a statistical model represent the relationship between each independent variable and the dependent variable, after accounting for other variables in the model. A social scientist needs not only to find statstical significant results; but understanding the underlying relationship these coefficients have in the real-world. For instance, in a linear regression model, a coefficient tells you how much the dependent variable is expected to change with a one-unit change in the independent variable, holding other variables constant. This information can be crucial for understanding the strength and nature of relationships in social science research.\n",
        "\n",
        "Most of the supervised learning models we saw in class fall in that definition:\n",
        "\n",
        "Linear Regression: Ideal for continuous outcomes; coefficients are directly interpretable, showing the expected change in the dependent variable with a one-unit change in the predictor.  Linear regression is fundamental in social science for its straightforward interpretability, making it a go-to method for understanding relationships between variables.\n",
        "\n",
        "Logistic Regression: Great for binary classification; coefficients can be transformed into odds ratios, offering a clear interpretation in terms of the odds of the outcome occurring. This model is widely used in social science when the dependent variable is categorical (e.g., success/failure, yes/no).\n",
        "\n",
        "Ridge and Lasso Regression: These are extensions of linear regression with regularization. They are useful for handling multicollinearity and overfitting. Their coefficients are interpretable in a similar way to linear regression, though the regularization can affect their magnitudes. It's important to note that Lasso regression can also help in feature selection due to its ability to shrink coefficients to zero, thereby identifying which variables are most important in predicting the outcome.\n",
        "\n",
        "Decision Trees: Can be used for both classification and regression. They are interpretable to some extent as they provide a clear decision path, but they can become complex with many branches.  They can be particularly useful for preliminary analysis to identify key variables and their interactions. However, they might lack stability, which is where ensemble methods like Random Forests come in.\n",
        "\n",
        "Random Forests: Mostly used for classification, but also applicable for regression. They offer good predictive power, but interpretability is lower compared to simpler models. They are beneficial when dealing with complex, non-linear relationships that simple models can't capture. The feature importance scores provided by Random Forests can be a valuable tool for understanding which variables are most influential in predicting the outcome.\n",
        "\n"
      ],
      "metadata": {
        "id": "hba9__7CtXcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Describe the main differences between supervised and unsupervised learning.\n",
        "\n",
        "Supervised learning is about learning, understanding and predicting the relationship of the explanatory x to the dependent y with the use of clear metrics, whereas unsupervised learning is about unravelling intricacies in the structure of the x dataset (subrgroups, unexplained variables, hidden effects) through PCA, clustering and manifold using interpretation on complex results. Here are the main differences, elaborated further:\n",
        "\n",
        "1. **Objective:**\n",
        "   - **Supervised Learning:** The primary goal is to learn a mapping from inputs (X) to outputs (Y), using labeled data where both the input and the output are provided. This is used for tasks such as regression (predicting continuous outcomes) and classification (predicting discrete labels).\n",
        "   - **Unsupervised Learning:** The focus here is on understanding the structure of the data (X) itself without any labeled outcomes. It involves uncovering hidden patterns, groupings, or structures in the data. This is used for tasks like clustering (finding natural groupings in data), dimensionality reduction (reducing the number of variables to simplify the model), and association (discovering rules that describe portions of the data).\n",
        "   - **Differences**: Supervised learning are easy to interpret as they are build in analyzing the x-y relationship within well defined objectives (classification of spam data, predicting number of petals for blooming flowers). Unsupervised learning can be more abstract and sometimes harder to interpret since it's about finding inherent structures or patterns in the data without predefined labels.\n",
        "\n",
        "2. **Data:**\n",
        "   - **Supervised Learning:** Requires a dataset where each instance of input data is labeled with the correct output. This labeling guides the learning process.\n",
        "   - **Unsupervised Learning:** Utilizes datasets without any explicit instructions on what to do with it. There are no predefined labels or outcomes associated with the data.\n",
        "   - **Differences**:Emphasizing the dependency of supervised learning on labeled data is crucial. Labeled data can be expensive or time-consuming to obtain, which is a significant limitation in some scenarios. In contrast, unsupervised learning can work with raw, unlabeled data, making it useful in situations where labeling data is impractical. The amount of avalaible data can have significant practical consideration in the choice between supervised and unsupervised learning approaches.\n",
        "\n",
        "3. **Examples of Techniques:**\n",
        "   - **Supervised Learning:** Includes algorithms like linear regression, logistic regression, support vector machines, neural networks, decision trees, and random forests.\n",
        "   - **Unsupervised Learning:** Common techniques include k-means clustering, hierarchical clustering, principal component analysis (PCA), and manifold learning with neural networks.\n",
        "   - **Differences**: Supervised learning fits the relationship in a linear approach; meanwhile, unsupervised uses techniques to fit multidimensional model to be interpreted in linear fashion. Keep in mind neural networks fit the data in a multidimensional approach with epoch training, but it still gives measures such as accuracy score with keras that otherwise be reserved in the realm of supervised linear learning.\n",
        "\n",
        "4. **Applications:**\n",
        "   - **Supervised Learning:** Used in scenarios where the outcome is known and the goal is to predict this outcome for new data. Examples include email spam classification, predicting house prices, or determining if a loan should be approved.\n",
        "   - **Unsupervised Learning:** Applied when the goal is to explore the data to find patterns or structure. This can be in market segmentation in business, gene sequence analysis in biology, or organizing large libraries of documents based on content.\n",
        "   - **Differences**: Supervised learning deals with situation in which we know the outcome we want to attain / predict. Unsupervised is about explaning the inner attributes and hidden patterns of the x factor so it could helps to visualise models more adequate in analyzing the relationhip x has with another y outcome currently undefined.\n",
        "\n",
        "\n",
        "5. **Evaluation:**\n",
        "   - **Supervised Learning:** Typically employs well-defined metrics for evaluation, such as accuracy, mean squared error (MSE), etc. These metrics are effective because the true output or labels are known, enabling direct measurement of the model's performance.\n",
        "   - **Unsupervised Learning:** Evaluation in unsupervised learning is more challenging due to the absence of ground truth labels. Metrics like the silhouette score in clustering or explained variance in PCA are used. These metrics help to assess the quality of the model's output, but they may not always correlate directly with real-world utility or interpretation.\n",
        "   - **Differences**: Supervised learning evaluations are relatively straightforward and directly linked to real-world outcomes due to the presence of labeled data. In contrast, unsupervised learning evaluation is often more subjective, focusing on the quality of data representation and the discovery of inherent patterns, which may not have immediate real-world interpretations.\n",
        "\n",
        "6. **Examples:**\n",
        "   - **Supervised Learning:** Typically employed in situations with clearly defined, labeled data. For example, predicting obesity rates in a dataset of 1000 individuals in New York, where each individual's attributes and obesity status are labeled.\n",
        "   - **Unsupervised Learning:** Often used for exploratory data analysis and finding hidden structures in unlabeled data. For instance, it can analyze customer transaction data to segment customers into groups with similar buying behaviors, enabling targeted marketing strategies without predefined categories.\n",
        "   - **Differences**: Supervised learning is primarily used for specific, objective-driven tasks like disease diagnosis or stock price prediction, where the relationships and outcomes are clearly defined. Unsupervised learning, however, excels in uncovering hidden patterns and structures in data, leading to exploratory insights that can inform a wide range of applications, from marketing strategies to understanding genetic data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fn_6upAyx0DR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Is supervised or unsupervised learning the primary approach that is used by machine learning practitioners?  For whatever approach you think is secondary, why would you use this approach (what's a good reason to use these kinds of models?)\n",
        "\n",
        "Based on the previous explanations, I believe supervised machine learning is easier to use and interpret thanks to easier metrics to understand the x-y relationship, and linear modelling that attempts to summarize the data in clear one-side direction. Unsupervised machine learning is a growing field of research, but its premises are necessary to understand the complex structure of variables effects on outcomes by investigating the hidden patterns in the variables.\n",
        "\n",
        "I would use the supervised method as my primary approach given that many practical problems are prediction problems, where the goal is to forecast an outcome based on input data. Supervised is very straightforward to uncover relations between x and y such as in classification tasks, which can be applied to a broad range of topics from image recognition to medical diagnosis. Supervised learning's straightforwardness comes from its clear structure: learning a mapping from known input-output pairs. This structure lends itself well to evaluation and optimization, as the performance of supervised learning models can be directly measured and compared. Supervised learning is often the primary approach in machine learning, particularly because it is directly applicable to a wide range of predictive tasks. These tasks are frequently encountered in various domains such as finance, healthcare, marketing, and more.\n",
        "\n",
        "Unsupervised is great in exploratory data analysis to ensure the inner validity of my hypothesis when the nature of the outcome is unknown or the variables are not clearly defined. It is a nice secondary approach when dealing in a complex environment; as my hypothesis could show varying results in my primary supervised approach as defining labels can be hard, I would use unsupervised methods to either reduce the data dimensions so it fits in a linear approach and detecting anomalies to understand the data inner attributes. It's important to highlight that unsupervised learning is not just a secondary choice but is often the primary method in situations where labeling data is impractical or impossible, such as in clustering and dimensionality reduction tasks. These situations are common in fields like genomics, anomaly detection, and market segmentation.\n",
        "\n",
        "In any of these scenario, unsupervised is useful to analyze what went wrong in the x structure, fixing such issues by uncovering hidden patterns, then training again the x dataset on the y variable but this time free of inner intracencies in the x-y relationship. For instance, clustering can reveal natural groupings in the data that were not previously considered, and dimensionality reduction techniques like PCA can help in identifying the most influential features. The model would then become more accurate, potentially more representative to reality. In some advanced applications, a combination of both supervised and unsupervised learning (sometimes referred to as semi-supervised learning) is used to leverage the strengths of both approaches. This is particularly useful when working with partially labeled datasets.\n",
        "\n",
        "In summary, while supervised learning is often the primary approach due to its direct applicability to prediction problems and ease of interpretation, unsupervised learning plays a crucial complementary role. It is particularly valuable for gaining a deeper understanding of the data, especially when supervised methods fall short or when exploring new, complex datasets.It's worth noting that unsupervised learning can provide insights that supervised learning might miss, such as identifying novel patterns or groups in the data that were not previously considered. The combination of both approaches can lead to more robust and insightful machine learning solutions. In other words, supervised approach could be seen as the actual coding leading to a real-life application whereas unsupervised is the debug mode to uncover the errors encountered in the coding."
      ],
      "metadata": {
        "id": "5d6sXphlzymy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Which unsupervised learning modeling approaches did we cover this semester?  What are the major differences between these techniques?\n",
        "\n",
        "Learned models: PCA, K-means clustering, Hierarchal clustering, Text analysis with TF-IDF vectorizer, Manifold learning with neural networks, convolutional neural networks (CNN):\n",
        "\n",
        "1. **Principal Component Analysis (PCA):**\n",
        "   - **Purpose:** PCA is a dimensionality reduction technique. It's used to reduce the number of variables in your data by transforming them into a new set of variables (principal components) that retain most of the original variance.\n",
        "   - **Application:** Ideal for simplifying complex datasets, visualization, and preprocessing before applying other machine learning techniques.\n",
        "   - **Key Difference:** PCA focuses on reducing the dimensionality of the data while maintaining as much variance as possible. It doesn’t involve any prediction or classification.\n",
        "\n",
        "2. **K-Means Clustering:**\n",
        "   - **Purpose:** This is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean.\n",
        "   - **Application:** Used for segmenting data into distinct groups based on similarities. Common in market segmentation, pattern recognition, and image compression.\n",
        "   - **Key Difference:** K-means is about grouping data points into clusters based on their features (similar to clouds). It requires the number of clusters to be specified a priori.\n",
        "\n",
        "3. **Hierarchical Clustering:**\n",
        "   - **Purpose:** This technique builds a hierarchy of clusters either in an agglomerative (bottom-up) or divisive (top-down) manner.\n",
        "   - **Application:** Useful for hierarchical data structures, like in biological taxonomy or organizational structures.\n",
        "   - **Key Difference:** Unlike k-means, it doesn’t require pre-specifying the number of clusters. It provides a dendrogram representing the nested grouping of patterns and similarity levels.\n",
        "\n",
        "4. **Text Analysis with TF-IDF Vectorizer:**\n",
        "   - **Purpose:** Term Frequency-Inverse Document Frequency (TF-IDF) is a numerical statistic intended to reflect how important a word is to a document in a collection or corpus.\n",
        "   - **Application:** Widely used in text mining and information retrieval; helps in understanding the relevance of words in documents.\n",
        "   - **Key Difference:** It's specific to text data and focuses on the significance of words/terms in the context of a document or a corpus.\n",
        "\n",
        "5. **Manifold Learning with Neural Networks:**\n",
        "   - **Purpose:** Manifold learning is about understanding the structure of high-dimensional data and finding low-dimensional representations that capture the essence of the data.\n",
        "   - **Application:** It's used in tasks that involve complex datasets, like image and speech recognition.\n",
        "   - **Key Difference:** This approach is often more complex and can capture non-linear relationships in the data.\n",
        "\n",
        "\n",
        "However, it's important to note that Convolutional Neural Networks (CNNs) are generally not considered an unsupervised learning technique. They are typically used in supervised learning settings, especially for image-related tasks. Here's a clarification and a few additional points:\n",
        "\n",
        "- -1. **Convolutional Neural Networks (CNNs):**\n",
        "   - **Purpose:** CNNs are primarily used in deep learning for analyzing visual imagery.\n",
        "   - **Application:** They have applications in image and video recognition, image classification, medical image analysis, etc.\n",
        "   - **Key Difference:** CNNs are specifically tailored for processing data that has a grid-like topology (like images). They use convolutional layers, making them different in architecture and functionality from standard neural networks. Unlike traditional neural networks, CNNs preserve the spatial structure of the data, making them particularly effective for image and video processing tasks."
      ],
      "metadata": {
        "id": "xMJ_4y7w3aMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What are the main benefits of using Principal Components Analysis?\n",
        "\n",
        "Principal Component Analysis (PCA) is primarily utilized for reducing the dimensionality of a dataset, focusing on retaining as much variance as possible while potentialy reducing the sample size observations. This process aims to simplify the dataset's structure, often into a form that is more linear, thereby facilitating the application of supervised learning techniques. Such simplification enhances the interpretability of the model, while also minimizing the loss of crucial information. This is done through identifying the principal components that capture the most variance in the data. This reduction helps in dealing with the \"curse of dimensionality,\" which can hamper the performance of many machine learning models.\n",
        "\n",
        "Essentially, PCA transforms a dataset with numerous dimensions into a series of linearly uncorrelated variables known as principal components, which are organized in a manner that captures the maximum variance. These principal components are linear combinations of the original variables and are orthogonal to each other (hence uncorrelated).  The first pca captures the most variance, followed by the second until n pca finds a linear combinations it cannot reduces anymore at the risk of losing variance. By reducing the number of features, PCA minimizes the complexity of the model, which can improve generalization to new data and avoid situation of overfitting in which the model innacurately increases the accuracy score due to mismatching new data to their labels (especially common in hierarchicals models).  PCA can improve the performance of machine learning models not only by reducing overfitting but also by removing noise and less informative variables, which can obscure meaningful relationships in simpler models\n",
        "\n",
        "Additionally, by eliminating redundant correlated variables and uncovering hidden patterns, PCA can enhance overall model performance by removing multicollinearity as the potentially correlated variables now become uncorrelated after dimensions are decreased, and reducing the dimensionality, leading to simpler and more efficient models.This is particularly beneficial for algorithms that do not perform well with high-dimensional data such as KNN to deal with reduced model, giving efficient insights as it would do on a dataset with a simple structure. PCA often leads to computational efficiency by reducing the number of dimensions, which can be especially beneficial when dealing with large datasets or when implementing complex algorithms\n",
        "\n",
        " A notable advantage of PCA is its efficacy in data visualization; it efficiently segregates observations in a two-to-three dimensional formats of distinct, meaningful groups that exhibit significant variances, thereby providing clearer insights into data patterns. PCA is very useful when dealing with a large dataset, potentially having a very complex structure.  It can be particularly useful in exploratory data analysis, where visualizing high-dimensional data in two or three dimensions can provide valuable insights.\n",
        "\n",
        " Finally, PCA can be used as a preprocessing step to transform the data into a more manageable form for further analysis using other techniques, particularly supervised learning models. By simplifying the data structure, PCA can make supervised learning models more effective and their results more interpretable, as you mentioned. Yet, while PCA reduces the number of features or dimensions, it does not reduce the number of observations. The dataset will have the same number of observations, but each will be described by fewer dimensions. In that way, PCA is particularly useful when dealing with large datasets, as it can significantly reduce the computational burden while maintaining the essential characteristics of the data.\n",
        "\n",
        " While discussing PCA, it's also useful to mention that PCA assumes linear relationships among variables and might not be effective for datasets where the underlying structure is non-linear.\n",
        "Additionally, while PCA can simplify data and make models more interpretable, the principal components themselves are often less interpretable in terms of the original features."
      ],
      "metadata": {
        "id": "cjJAVXEd8KyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Thinking about neural networks, what are three major differences between a deep multilayer perceptron network and a convolutional neural network model?  Be sure to define any key terms in your explanation.\n",
        "\n",
        "Neural networks are used when the use of linear models is irrelevant in a dataset. Let's assume we have a dataset with a circle-like distribution. Drawing a 45 degree best-fitting line that attempts to capture most of the point would be imposible as the data-structure cannot become linear in any way. Neural networks are specifically executed on non-linear models, and can find the hidden relationships or patterns that traditional linear models would miss otherwise. Neural networks, in general, are powerful tools for modeling complex, non-linear relationships in data, making them suitable for a wide range of tasks beyond the capabilities of linear models.\n",
        "\n",
        "A deep multilayer perceptron network, referred as MLP, consists of at least three layers of nodes: an input layer, an outpur layer and a hidden layer. With the notable exception of the input layer, each node is a neuron with nonlinear features. MLP has the disadvantages of having multiple fully connected layers which implies the number of parameters can get very high due to mutually enforced effects. Finally, it takes flattened vectors as inputs\n",
        "\n",
        "Convutional Neural Networks (CNN) account for local connectivity instaed to mutual agreed features accross nodes. Each layer is filtered for each dimension of an image (meaning it accounts for linear dimensions but like higher dimensions with different physics) enabling to recognise different patterns accross the image no matter the size. Input layers take matrices as well as vectors, which can go deeper than the flattned level.  Layers are sparsely connected rather than fully connected like in MLP. Every nodes is connected to necessarily another node. CNN essentially allows parameter sharing, weight sharing so that the filter looks for a specific pattern, and is location invariant — can find the pattern anywhere in an image. This is very useful for object detection. Patterns can be discovered in more than one part of the image.\n",
        "\n",
        "\n",
        "Let's summarize this observation into three differences:\n",
        "\n",
        "\n",
        "1. **Structure and Connectivity:**\n",
        "   - **MLP:** An MLP consists of an input layer, one or more hidden layers, and an output layer, all of which are connected. This structure can lead to a large number of parameters, especially as the number of layers (depth) and neurons (width) increases. In MLP, each layer is fully connected to the next layer, leading to a high number of parameters, especially in deep networks. This full connectivity makes MLPs versatile but also computationally intensive.\n",
        "   - **CNN:** CNNs, particularly designed for processing data with a grid-like topology (like images), use a different kind of layer called a convolutional layer. In these layers, neurons are not fully connected to all neurons in the previous layer but only to a small region of it. This local connectivity allow CNNs to focus on local patterns (like edges in images) and reduce the number of parameters significantly. Convolutional layers use filters to perform convolution operations, capturing local features like edges and textures in images, bringing unparraled success in tasks like image recognition.\n",
        "\n",
        "2. **Parameter Sharing and Weight Sharing:**\n",
        "   - **MLP:** In MLPs, there is no parameter sharing; each weight is used only for a specific connection between two neurons. Parameters are irrelevant as the MLP processes the same features accross the output and hidden layers with no varying differences, leading the model to be somehow inflexible. The lack of parameter sharing in MLPs results in a model that is more prone to overfitting, especially with complex datasets.\n",
        "   - **CNN:** CNNs use the concept of weight sharing, where the same filter is applied across different parts of the input. This not only reduces the number of parameters but also makes CNNs translation invariant as it recognizes patterns regardless of their position in the input space.  This makes CNN more efficient in terms of the number of parameters and more effective in detecting features regardless of their location in the input space.\n",
        "\n",
        "3. **Input Data Structure and Processing:**\n",
        "   - **MLP:** MLPs take input as flat vectors. Each input feature is considered independent, and spatial relationships between pixels in images are not maintained. This significantly reduces the ability of the model to recognizes hidden paterns when the picture scale is very smale; where each attempt to reproduce the picture will often be unsuccesful. The lack of spatial awareness in MLPs makes them less suitable for tasks where spatial relationships are key, such as image and video analysis.\n",
        "   - **CNN:** CNNs take inputs as multi-dimensional arrays (like 2D arrays for grayscale images or 3D arrays for color images). This allows them to maintain and utilize the spatial hierarchy and relationships in the data. CNNs are particularly adept at handling image data where the proximity of pixels and the patterns they form are crucial. CNN preserve the spatial structure of the input data, allowing them to efficiently process and interpret image data.\n",
        "\n",
        "In summary, while MLPs are a more general form of neural networks suitable for a wide range of tasks, CNNs are specialized for tasks that benefit from recognizing local patterns and maintaining spatial relationships in the data, such as image and video recognition tasks. The structural differences between MLPs and CNNs cater to their respective strengths and use cases, with MLPs being more straightforward but parameter-heavy, and CNNs being more efficient and effective for grid-like data. While MLPs are versatile and can be used for a variety of tasks, they are particularly well-suited for structured data or datasets where spatial relationships are less important, CNNs are specifically designed for image processing and other tasks where recognizing spatial hierarchies and patterns is crucial.\n",
        "\n"
      ],
      "metadata": {
        "id": "rzXi5J9fLHVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Three hidden layers.  50 hidden units in the first hidden layer, 100 in the second, and 150 in the third.  Activate all hidden layers with relu.  The output layer should be built to classify to five categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
      ],
      "metadata": {
        "id": "eeNY0YCwS-K4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dense(5, activation='softmax') line in tge model architecture sets up an output layer with 5 units. When using the softmax activation function, these 5 units correspond to the probability distribution over 5 different classes. Each unit in this layer will output a probability for one of the classes, and the class with the highest probability will be the model's prediction.\n",
        "\n",
        "In a classification setting, each unit in this output layer represents one category or class. Since you're using the softmax activation function, the model will output a probability distribution over these 5 classes. Each of the 5 units will output a probability score, and these scores will sum up to 1. The class associated with the unit that has the highest probability score will be the model's prediction.\n",
        "\n",
        "- The input_dim parameter in the first Dense layer must be set according to the number of features in your input dataset.\n",
        "\n",
        "- The output layer uses a softmax activation function, suitable for multi-class classification into five categories.\n",
        "- The model is compiled with Stochastic Gradient Descent (SGD) as the optimizer. I've set a learning rate of 0.01, but you might need to adjust this based on your specific problem.\n",
        "- The loss function is set to categorical_crossentropy, which is commonly used for multi-class classification problems.\n",
        "We also track the accuracy metric during training."
      ],
      "metadata": {
        "id": "HKoa4wdgVEAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(50, activation='relu', input_dim=<input_dimension>))  # First hidden layer, adjust input_dim to match your dataset\n",
        "model.add(Dense(100, activation='relu'))  # Second hidden layer\n",
        "model.add(Dense(150, activation='relu'))  # Third hidden layer\n",
        "model.add(Dense(5, activation='softmax'))  # Output layer for 5-class classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "LMULP2dLT2_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Two hidden layers.  75 hidden units in the first hidden layer and 150 in the second.  Activate all hidden layers with relu.  The output layer should be built to classify a binary dependent variable.  Further, your optimization technique should be stochastic gradient descent. (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
      ],
      "metadata": {
        "id": "fElh667rVeQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output layer has 1 unit with a sigmoid activation function, which is typical for binary classification. The sigmoid function outputs a value between 0 and 1. This is particularly useful in binary classification, where we typically want to predict the probability of an input belonging to one of two classes, commonly referred to as the \"positive\" class (often labeled as 1) and the \"negative\" class (often labeled as 0). By producing a probability (a value between 0 and 1), the sigmoid function allows the model to express a degree of certainty about whether the input belongs to the positive class.\n",
        "\n",
        "The sigmoid function, defined as σ(x) = 1 / (1 + e^(-x)), has a characteristic \"S\"-shaped curve. This shape is ideal for binary classification as it provides a smooth gradient that can be applied to any real-valued number, compressing it into a range between 0 and 1. The gradient of the sigmoid function is strongest around x = 0 and diminishes as x moves away from 0. This property ensures that changes are most responsive when the predictions are uncertain (around 0.5) and less responsive as they become more confident (approaching 0 or 1). This is useful during the training process of a neural network.\n",
        "\n",
        "The loss function binary_crossentropy is used instead of categorical_crossentropy as it is more suitable for binary classification tasks.\n",
        "\n",
        "- Replace <input_dimension> with the number of features in your input data. This is crucial for the model to understand the shape of the input it should expect.\n",
        "- The output layer uses a sigmoid activation function to output a probability score between 0 and 1, suitable for binary classification.\n",
        "- The binary_crossentropy loss function is used for binary classification, in contrast to categorical_crossentropy which is used for multi-class classification.\n",
        "- I've included a learning rate of 0.01 for the SGD optimizer, but this can be adjusted based on your specific needs.\n",
        "- The model summary will provide an overview of your neural network, including the architecture and the number of parameters in each layer."
      ],
      "metadata": {
        "id": "tszAH57gVyNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(75, activation='relu', input_dim=<input_dimension>))  # First hidden layer, adjust input_dim to match your dataset\n",
        "model.add(Dense(150, activation='relu'))  # Second hidden layer\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=SGD(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "lUEn75vLVe4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Write the tf.keras code for a convolutional neural network with the following structure: Two convolutional layers.  16 filters in the first layer and 28 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  The output layer should be built to classify to ten categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
      ],
      "metadata": {
        "id": "7GteR8Y0WI4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is a single Sequential instance. Layers are added sequentially to this model.\n",
        "The first Conv2D layer specifies the input_shape, which should match the shape of your input data.\n",
        "The Conv2D layers use 16 and 28 filters, respectively, with a kernel size of (3, 3).\n",
        "MaxPooling2D layers with a pool size of (2, 2) are used after each convolutional layer.\n",
        "After flattening the output, a Dense layer with 10 units is added for the output, with softmax activation, suitable for multi-class classification (10 classes in this case).\n",
        "The model is compiled with SGD optimizer and categorical_crossentropy loss function, standard for multi-class classification tasks.\n",
        "Remember to replace <height>, <width>, and <channels> with the actual dimensions of your input data when using this model with real data.\n",
        "\n",
        "\n",
        "- Replace <height>, <width>, and <channels> in the input_shape parameter of the first Conv2D layer with the actual dimensions of your input data. This is crucial for the model to understand the shape of the input it should expect.\n",
        "- The use of relu activation in convolutional layers and softmax in the output layer is appropriate for this type of network and task.\n",
        "- The learning rate for the SGD optimizer is set to 0.1, but you might need to adjust this based on your specific problem and dataset.\n",
        "- The model summary will provide a comprehensive overview of your CNN, including the architecture and the number of parameters at each layer."
      ],
      "metadata": {
        "id": "E0-aoJd8ZrM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Create the Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# First convolutional layer with 16 filters and relu activation\n",
        "model.add(Conv2D(16, (3, 3), padding='valid', input_shape=(<height>, <width>, <channels>)))  # Replace <height>, <width>, <channels> with your input shape dimensions\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second convolutional layer with 28 filters and relu activation\n",
        "model.add(Conv2D(28, (3, 3), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flattening the layer and adding dense layer for classification\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))  # Output layer for 10-class classification\n",
        "\n",
        "# Compiling the model\n",
        "sgd = SGD(learning_rate=0.1)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "ri03pJRTWMjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Write the keras code for a convolutional neural network with the following structure: Two convolutional layers.  32 filters in the first layer and 32 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  Add two fully connected layers with 128 hidden units in each layer and relu activations.  The output layer should be built to classify to six categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
      ],
      "metadata": {
        "id": "4ILAYYs2Zst1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each convolutional layer is followed by a max pooling layer, and then the output is flattened before being fed to the fully connected layers.\n",
        "Only one Flatten layer is needed right before the fully connected layers.\n",
        "The dense layers for classification are consolidated into one sequence, with the final dense layer having 6 units corresponding to the 6 categories.\n",
        "The activation parameter is included directly in the Conv2D and Dense layers for clarity and conciseness.\n",
        "Remember to replace <height>, <width>, and <channels> with the actual dimensions of your input data when using this model with real data.\n",
        "\n",
        "The Dense layers (fully connected layers) should come after the Flatten layer, not interspersed between the convolutional layers. This is because the Dense layers expect a flattened input. Here's the corrected version of your code:\n",
        "\n",
        "The two convolutional layers with 32 filters each are followed by max pooling layers.\n",
        "After the convolutional and pooling layers, the Flatten layer is used to convert the 2D feature maps into a 1D vector.\n",
        "This is followed by two fully connected (Dense) layers with 128 units each.\n",
        "Finally, the output layer with 6 units and a softmax activation function is used for classification into six categories.\n",
        "The model is compiled with the SGD optimizer and categorical_crossentropy loss function, suitable for a multi-class classification task.\n",
        "Remember to replace <height>, <width>, and <channels> with the actual dimensions of your input data when using this model with real data.\n",
        "\n",
        "- Make sure to replace <height>, <width>, and <channels> in the input_shape parameter of the first Conv2D layer with the actual dimensions of your input data.\n",
        "- The relu activation function is used in both convolutional and fully connected layers, making the model capable of learning non-linear relationships.\n",
        "- The final Dense layer with 6 units and softmax activation is suitable for classifying the inputs into one of six categories.\n",
        "- The learning rate for the SGD optimizer is set at 0.1. Depending on your dataset and specific task, you may need to adjust this value."
      ],
      "metadata": {
        "id": "Rk0vshNeakGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# First convolutional layer with 32 filters and relu activation\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='valid', input_shape=(<height>, <width>, <channels>)))  # Replace <height>, <width>, <channels> with your input shape dimensions\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second convolutional layer with 32 filters and relu activation\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='valid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten the output of the pooling layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# First fully connected layer with 128 hidden units\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Second fully connected layer with 128 hidden units\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Output layer with 6 units (for 6 categories) and softmax activation\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "# Compile the model with SGD optimizer\n",
        "sgd = SGD(learning_rate=0.1)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "H4TKSOBCZvXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html /content/Final ML.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVhk4gCqnKQh",
        "outputId": "55016e47-43d7-4892-d417-114580883ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] WARNING | pattern '/content/Final' matched no files\n",
            "[NbConvertApp] WARNING | pattern 'ML.ipynb' matched no files\n",
            "This application is used to convert notebook files (*.ipynb)\n",
            "        to various other formats.\n",
            "\n",
            "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "=======\n",
            "The options below are convenience aliases to configurable class-options,\n",
            "as listed in the \"Equivalent to\" description-line of the aliases.\n",
            "To see all configurable class-options for some <cmd>, use:\n",
            "    <cmd> --help-all\n",
            "\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "    Equivalent to: [--Application.log_level=10]\n",
            "--show-config\n",
            "    Show the application's configuration (human-readable format)\n",
            "    Equivalent to: [--Application.show_config=True]\n",
            "--show-config-json\n",
            "    Show the application's configuration (json format)\n",
            "    Equivalent to: [--Application.show_config_json=True]\n",
            "--generate-config\n",
            "    generate default config file\n",
            "    Equivalent to: [--JupyterApp.generate_config=True]\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only\n",
            "            relevant when converting to notebook format)\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
            "--clear-output\n",
            "    Clear output of current file and save in place,\n",
            "            overwriting the existing notebook.\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document.\n",
            "            This mode is ideal for generating code-free reports.\n",
            "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n",
            "--allow-chromium-download\n",
            "    Whether to allow downloading chromium if no suitable version is found on the system.\n",
            "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n",
            "--disable-chromium-sandbox\n",
            "    Disable chromium security sandbox when converting to PDF..\n",
            "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n",
            "--show-input\n",
            "    Shows code input. This flag is only useful for dejavu users.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input=False]\n",
            "--embed-images\n",
            "    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n",
            "    Equivalent to: [--HTMLExporter.embed_images=True]\n",
            "--sanitize-html\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html=True]\n",
            "--log-level=<Enum>\n",
            "    Set the log level by value or name.\n",
            "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
            "    Default: 30\n",
            "    Equivalent to: [--Application.log_level]\n",
            "--config=<Unicode>\n",
            "    Full path of a config file.\n",
            "    Default: ''\n",
            "    Equivalent to: [--JupyterApp.config_file]\n",
            "--to=<Unicode>\n",
            "    The export format to be used, either one of the built-in formats\n",
            "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides', 'webpdf']\n",
            "            or a dotted object name that represents the import path for an\n",
            "            ``Exporter`` class\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.export_format]\n",
            "--template=<Unicode>\n",
            "    Name of the template to use\n",
            "    Default: ''\n",
            "    Equivalent to: [--TemplateExporter.template_name]\n",
            "--template-file=<Unicode>\n",
            "    Name of the template file to use\n",
            "    Default: None\n",
            "    Equivalent to: [--TemplateExporter.template_file]\n",
            "--theme=<Unicode>\n",
            "    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n",
            "    as prebuilt extension for the lab template)\n",
            "    Default: 'light'\n",
            "    Equivalent to: [--HTMLExporter.theme]\n",
            "--sanitize_html=<Bool>\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n",
            "    should be set to True by nbviewer or similar tools.\n",
            "    Default: False\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html]\n",
            "--writer=<DottedObjectName>\n",
            "    Writer class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: 'FilesWriter'\n",
            "    Equivalent to: [--NbConvertApp.writer_class]\n",
            "--post=<DottedOrNone>\n",
            "    PostProcessor class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
            "--output=<Unicode>\n",
            "    overwrite base name use for output files.\n",
            "                can only be used when converting one notebook at a time.\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.output_base]\n",
            "--output-dir=<Unicode>\n",
            "    Directory to write output(s) to. Defaults\n",
            "                                  to output to the directory of each notebook. To recover\n",
            "                                  previous default behaviour (outputting to the current\n",
            "                                  working directory) use . as the flag value.\n",
            "    Default: ''\n",
            "    Equivalent to: [--FilesWriter.build_directory]\n",
            "--reveal-prefix=<Unicode>\n",
            "    The URL prefix for reveal.js (version 3.x).\n",
            "            This defaults to the reveal CDN, but can be any url pointing to a copy\n",
            "            of reveal.js.\n",
            "            For speaker notes to work, this must be a relative path to a local\n",
            "            copy of reveal.js: e.g., \"reveal.js\".\n",
            "            If a relative path is given, it must be a subdirectory of the\n",
            "            current directory (from which the server is run).\n",
            "            See the usage documentation\n",
            "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
            "            for more details.\n",
            "    Default: ''\n",
            "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
            "--nbformat=<Enum>\n",
            "    The nbformat version to write.\n",
            "            Use this to downgrade notebooks.\n",
            "    Choices: any of [1, 2, 3, 4]\n",
            "    Default: 4\n",
            "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to html\n",
            "\n",
            "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides', 'webpdf'].\n",
            "\n",
            "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "\n",
            "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n",
            "            'classic'. You can specify the flavor of the format used.\n",
            "\n",
            "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n",
            "\n",
            "            You can also pipe the output to stdout, rather than a file\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "\n",
            "            PDF is generated via latex\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "\n",
            "            You can get (and serve) a Reveal.js-powered slideshow\n",
            "\n",
            "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "\n",
            "            Multiple notebooks can be given at the command line in a couple of\n",
            "            different ways:\n",
            "\n",
            "            > jupyter nbconvert notebook*.ipynb\n",
            "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "\n",
            "            or you can specify the notebooks list in a config file, containing::\n",
            "\n",
            "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "\n",
            "            > jupyter nbconvert --config mycfg.py\n",
            "\n",
            "To see all available configurables, use `--help-all`.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}