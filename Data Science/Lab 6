#--- R Lab # 6 ----
library(plm)
library(lmtest)
setwd("/Users/jean/Desktop")
d = read.csv("panel-for-R.csv")
vars <- c("idnum","panelwave","educ","race","year") #
pd.sub <- d[, vars]
pd.sub$lneduc = log(pd.sub$educ)
pd.sub$lnrace = log(pd.sub$race)
## 1. Run a naive ("pooled") OLS regression on the panel data. Tell we how you expect your Xs to affect your Y and why. Apply clustered standard errors too. Interpret your results.
lm1 <- lm(educ ~ as.factor(race), data = pd.sub)
summary(lm1)
## Here is the clusterSE from the QMSS package ##
#' Compute clustered standard errors.
#'
#' @param fit A model fit with \code{\link[plm]{plm}} (\pkg{plm}).
#' @param cluster.var A character string naming the grouping/cluster variable.
#' @param data A data frame containing \code{cluster.var} Only needed if
#' \code{cluster.var} is not included in \code{index}. See 'Examples' below.
#' @return Output from \code{\link[lmtest]{coeftest}} (\pkg{lmtest}) but with clustered standard errors.
#' @author Jonah Gabry <jsg2201@@columbia.edu>
#' @note \code{clusterSE} does not work with models fit with \code{lm}, however a similar model
#' can be fit with \code{\link[plm]{plm}} using the option \code{model = "pooling"}. You can then
#' use \code{clusterSE} to compute clustered standard errors and retest the coefficients.
#' @seealso \code{\link[lmtest]{coeftest}}
#' @export
#' @examples
#' \dontrun{
#' # Model from plm help page:
#' data("Produc", package = "plm")
#' fit <- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,
#' data = Produc, index = c("state","year"), model = "random")
#'
#' clusterSE(fit, cluster.var = "state") # don't need data argument since "state" is included in index
#' }
#'
clusterSE <- function(fit, cluster.var, data){ # note: cluster.var should be entered as character string
require(plm); require(lmtest)
if (missing(data) & cluster.var %in% colnames(index(fit))){
cvar <- index(fit, cluster.var)
n <- length(unique(cvar))
N <- length(cvar)
}
else{
row.ids <- as.numeric(rownames(model.frame(fit)))
# 1. get number of clusters (omitting individuals with missingness on "divorce.easier" and/or "divorced")
n <- length(unique(data[row.ids, cluster.var]))
# 2. get number of observations (again omitting the same individuals with missingness)
N <- length(row.ids)
}
#3. compute degrees of freedom
df <- (n/(n - 1)) * (N - 1)/fit$df.residual
# compute variance-covariance matrix
vcov <- df*vcovHC(fit, type = "HC0", cluster = "group")
# retest coefficients
coeftest(fit, vcov = vcov)
}
clusterSE(fit = lm1, cluster.var = "idnum", data=pd.sub)
## What if we run the same model as an elasticity, log-log model? We regress log hours on log income.
## This is also quite statistically significant, where for every 1% increase in hours worked, on average, we see a .83% increase in money. Look at that increase in R-sq, now explaining almost 14% of the variance in log salary.
#lm2 <- lm(lneduc ~ lnrace + as.factor(panelwave), data = pd.sub)
#summary(lm2)
#clusterSE(fit = lm2, cluster.var = "idnum", data=pd.sub)
## 2. Run a first differences regression on the same model in Question 1. Interpret your results. Do you draw a different conclusion than in Question 1? Explain.
plm1 <- plm(educ ~ as.factor(race) + year, index = c("idnum", "panelwave"), model = "fd", data = pd.sub)
summary(plm1)
clusterSE(fit = plm1, cluster.var = "idnum", data=pd.sub
